x1 <- c(1,1,0,5,6,4)
x2 <- c(4,3,4,1,2,0)
X <- cbind(x1, x2)
X

plot(X[,1], X[,2])


set.seed(1)
clusters <- sample(c(1,2), nrow(X), replace = TRUE)
clusters

plot(X[, 1], X[, 2], col = (clusters + 1), pch = 20, cex = 2)

#compute the centroid
centroid1 <- c(mean(X[clusters == 1, 1]), mean(X[clusters == 1, 2]))
centroid2 <- c(mean(X[clusters == 2, 1]), mean(X[clusters == 2, 2]))
plot(X[, 1], X[, 2], col = (clusters + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)

# assign each point to it's closest centroid
clusters <- c(2, 2, 2, 1, 1, 1)
plot(X[, 1], X[, 2], col = (clusters + 1), pch = 20, cex = 2)

centroid1 <- c(mean(X[clusters == 1, 1]), mean(X[clusters == 1, 2]))
centroid2 <- c(mean(X[clusters == 2, 1]), mean(X[clusters == 2, 2]))
plot(X[, 1], X[, 2], col = (clusters + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)


################################
# Q8
################################
dat <- USArrests
#a
pr.out <- prcomp(dat, scale = TRUE)
pr.var <- pr.out$sdev^2
pve <- pr.var / sum(pr.var)
sum(pr.var)
pve
#b
loadings <- pr.out$rotation
USArrests2 <- scale(USArrests)
sumvar <- sum(apply(as.matrix(USArrests2)^2, 2, sum))
apply((as.matrix(USArrests2) %*% loadings)^2, 2, sum) / sumvar



################################
# Q9
################################
# a.  Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.
hc.complete <- hclust(dist(dat), method = "complete")
plot(hc.complete)
# b. Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?
cutree(hc.complete, 3)
# c. Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one
dat.scaled <- scale(dat)
hc.complete.scaled <- hclust(dist(dat.scaled), method = "complete")
plot(hc.complete.scaled)
# What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be
# scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer.
cutree(hc.complete.scaled, 3)

table(cutree(hc.complete, 3), cutree(hc.complete.scaled, 3))

# Scaling the variables affect the clusters obtained although the trees are somewhat similar. The variables should
# be scaled beforehand because the data measures have different units.
